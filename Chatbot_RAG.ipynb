{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41a7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "import polars as pl\n",
    "from pypdf import PdfReader\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "#from google import generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdd686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key= os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619963a",
   "metadata": {},
   "source": [
    "To test API key is working or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5d6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents = \"Hello,who am I talking to?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459a482",
   "metadata": {},
   "source": [
    "Extract sentences from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f13ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "def extract_sentences_from_pdf(pdf_path, start=6, end=124):\n",
    "    \"\"\"Extract sentences from PDF with metadata (page number & sentence index).\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    sentences_with_meta = []\n",
    "\n",
    "    for i in range(max(0, start - 1), min(end, len(reader.pages))):\n",
    "        page_text = reader.pages[i].extract_text() or \"\" \n",
    "        sentences = nltk.sent_tokenize(page_text) # Splits the text into idividual sentences\n",
    "\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            sentences_with_meta.append({\"text\": sentence, \"meta\": {\"type\": \"sentence\", \"page\": i + 1, \"index\": idx}})\n",
    "\n",
    "    return sentences_with_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8314db",
   "metadata": {},
   "source": [
    "Creating Embedding for Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41df1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"models/embedding-001\", task_type=\"SEMANTIC_SIMILARITY\"):\n",
    "    \"\"\"Generate embeddings for a given text with error handling.\"\"\"\n",
    "    try:\n",
    "        response = client.models.embed_content(\n",
    "            model=model,\n",
    "            contents=text,\n",
    "            config=types.EmbedContentConfig(task_type=task_type)\n",
    "        )\n",
    "        return response.embeddings[0].values  # Return vector directly\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding Error: {e}\")\n",
    "        return np.zeros(768)  # Fallback vector to prevent failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f8bac",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bca899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667c5a3",
   "metadata": {},
   "source": [
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c886451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Store embeddings and metadata for retrieval.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add(self, text, vector, meta):\n",
    "        \"\"\"It allows the class to grow dynamically as new embeddings are added\"\"\"\n",
    "        self.vectors.append(np.array(vector))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(meta)\n",
    "\n",
    "    def semantic_search(self, query_vector, k=10):\n",
    "        \"\"\"Retrieve the top-k most relevant sentences using semantic search.\"\"\"\n",
    "        scores = [(i, cosine_similarity(query_vector, v)) for i, v in enumerate(self.vectors)]\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [{\"text\": self.texts[i], \"meta\": self.metadata[i]} for i, _ in scores[:k]]\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\"Save vectors, texts, and metadata.\"\"\"\n",
    "        df = pl.DataFrame({\"vectors\": self.vectors, \"texts\": self.texts, \"metadata\": self.metadata})\n",
    "        df.write_parquet(file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"Load stored embeddings and metadata.\"\"\"\n",
    "        df = pl.read_parquet(file_path)\n",
    "        self.vectors = df[\"vectors\"].to_list()\n",
    "        self.texts = df[\"texts\"].to_list()\n",
    "        self.metadata = df[\"metadata\"].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d57fc",
   "metadata": {},
   "source": [
    "Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cb49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, matched_sentences):\n",
    "    \"\"\"Generate a response using retrieved contextual sentences.\"\"\"\n",
    "    if not matched_sentences:\n",
    "        return \"I don't know.\"\n",
    "\n",
    "    context = \"\\n\".join([entry[\"text\"] for entry in matched_sentences])\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Use the provided context to answer the user's question.\\n\"\n",
    "        \"If the answer is clearly stated or implied in the context, provide it concisely.\\n\"\n",
    "        \"If it's not found in the context, say 'I don't know.' Do not make up information.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=f\"Question: {query}\\n\\nContext:\\n{context}\",\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt) #ensure AI follows specific rules when answering\n",
    "        )\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return \"Error: Unable to generate response.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ad499",
   "metadata": {},
   "source": [
    "Evaluate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4d5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(query, ai_answer, ideal_answer):\n",
    "    system_prompt = (\n",
    "        \"You are an evaluation system.\\n\"\n",
    "        \"Score the assistant's answer as follows:\\n\"\n",
    "        \"- 1 if it is correct and complete\\n\"\n",
    "        \"- 0.5 if it is partially correct\\n\"\n",
    "        \"- 0 if it is incorrect or missing\\n\"\n",
    "        \"Also provide a brief justification.\"\n",
    "    )\n",
    "    eval_prompt = f\"Question: {query}\\nAI Answer: {ai_answer}\\nIdeal Answer: {ideal_answer}\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[eval_prompt],\n",
    "        config=types.GenerateContentConfig(system_instruction=system_prompt, temperature=0.4)\n",
    "    )\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93740f",
   "metadata": {},
   "source": [
    "Vector_store(\"embeddings.paraquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a52437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vector_store(store, file_path=\"embeddings.parquet\"):\n",
    "    \"\"\"Save embeddings for reuse.\"\"\"\n",
    "    store.save(file_path)\n",
    "    print(f\"Saved vector store to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebb3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(file_path=\"embeddings.parquet\"):\n",
    "    \"\"\"Load stored embeddings instead of regenerating them.\"\"\"\n",
    "    store = VectorStore()\n",
    "    store.load(file_path)\n",
    "    print(f\"Loaded vector store from {file_path}\")\n",
    "    return store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bddc31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬──────────────────┐\n",
      "│ vectors                         ┆ texts                           ┆ metadata         │\n",
      "│ ---                             ┆ ---                             ┆ ---              │\n",
      "│ array[f64, 768]                 ┆ str                             ┆ struct[3]        │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪══════════════════╡\n",
      "│ [0.005048, -0.057885, … 0.0119… ┆ Introduction                    ┆ {\"sentence\",6,0} │\n",
      "│                                 ┆ Fundamental traff…              ┆                  │\n",
      "│ [-0.003776, -0.061707, … -0.01… ┆ Be considerate of those using … ┆ {\"sentence\",6,1} │\n",
      "│ [0.017079, -0.053181, … 0.0257… ┆ Be especially considerate of c… ┆ {\"sentence\",6,2} │\n",
      "│ [-0.009283, -0.038325, … 0.028… ┆ Do not cause any unnecessary d… ┆ {\"sentence\",6,3} │\n",
      "│ [0.020128, 0.005779, … -0.0170… ┆ No-one has any rights, only jo… ┆ {\"sentence\",6,4} │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pl.read_parquet(\"embeddings.parquet\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd2a940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentences from PDF...\n",
      "Creating embeddings for sentences...\n",
      "Saved vector store to embeddings.parquet\n",
      "Loaded vector store from embeddings.parquet\n",
      "Vector store successfully loaded!\n",
      "Total stored vectors: 1023\n",
      "\n",
      "Loading saved vector store for search...\n",
      "Loaded vector store from embeddings.parquet\n",
      "\n",
      "Top matched sentences for context:\n",
      "- This means that the pedestrian crossing is uncontrolled. (Page 47, Sentence 4)\n",
      "- An uncontrolled pedestrian crossing. (Page 47, Sentence 0)\n",
      "- Uncontrolled pedestrian crossings\n",
      "Drivers have an obligation to give way to pedestrians who have stepped\n",
      "out onto the pedestrian crossing or who are about to do so. (Page 46, Sentence 12)\n",
      "- It is absent from most intersections where the priority-to-the-right\n",
      "rule applies. (Page 32, Sentence 2)\n",
      "- Intersecting traffic have a red light,\n",
      "but oncoming traffic might have a\n",
      "green light. (Page 41, Sentence 1)\n",
      "- If there is no stop line, stop just\n",
      "before entering the intersecting road. (Page 24, Sentence 5)\n",
      "- Driver A must let you pass according to the turning rule, B must give\n",
      "way to A according to the priority-to-the-right rule, you must give way to B\n",
      "according to the priority-to-the-right rule, and nobody is allowed to enter the\n",
      "intersection if they risk having to stop in the middle of the intersection (the\n",
      "obstruction rule). (Page 22, Sentence 5)\n",
      "- Obligation to give way\n",
      "Let intersecting vehicle traffic pass. (Page 23, Sentence 0)\n",
      "- Priority-to-the-right rule, example 3\n",
      "B\n",
      "A\n",
      "The roads do not have to intersect at a 90° angle. (Page 29, Sentence 0)\n",
      "- This rule applies even if A has a\n",
      "green light (B can have a green light at the same time). (Page 33, Sentence 3)\n",
      "\n",
      "=== Question ===\n",
      "What is the rule at an uncontrolled intersection?\n",
      "\n",
      "=== Answer ===\n",
      "The priority-to-the-right rule applies at an uncontrolled intersection.\n",
      "\n",
      "Evaluation:\n",
      "Score: 1\n",
      "Justification: The AI's answer is correct and complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"Driving_theory_book_2025.pdf\"\n",
    "\n",
    "    print(\"Extracting sentences from PDF...\")\n",
    "    sentences = extract_sentences_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"Creating embeddings for sentences...\")\n",
    "    store = VectorStore()\n",
    "\n",
    "    for i, entry in enumerate(sentences):\n",
    "        emb = create_embeddings(entry[\"text\"])\n",
    "        store.add(entry[\"text\"], emb, entry[\"meta\"])\n",
    "        time.sleep(0.5)  # Reduced delay for faster execution\n",
    "\n",
    "    save_vector_store(store)\n",
    "\n",
    "    vector_store = load_vector_store(\"embeddings.parquet\")  # Load stored embeddings\n",
    "    print(\"Vector store successfully loaded!\")  # Confirm it works\n",
    "    print(f\"Total stored vectors: {len(vector_store.vectors)}\") # Check stored embeddings count\n",
    "\n",
    "    # Sample query\n",
    "    question = \"What is the rule at an uncontrolled intersection?\"\n",
    "    query_vector = create_embeddings(question)\n",
    "\n",
    "    print(\"\\nLoading saved vector store for search...\")\n",
    "    store = load_vector_store()\n",
    "\n",
    "    top_sentences = store.semantic_search(query_vector, k=10)\n",
    "\n",
    "    print(\"\\nTop matched sentences for context:\")\n",
    "    for s in top_sentences:\n",
    "        print(f\"- {s['text']} (Page {s['meta']['page']}, Sentence {s['meta']['index']})\")\n",
    "\n",
    "    answer = generate_answer(question, top_sentences)\n",
    "\n",
    "    print(\"\\n=== Question ===\")\n",
    "    print(question)\n",
    "    print(\"\\n=== Answer ===\")\n",
    "    print(answer)\n",
    "\n",
    "    ideal_answer = \"You must yield to traffic coming from the right.\"\n",
    "\n",
    "    print(\"\\nEvaluation:\")\n",
    "    print(evaluate_answer(question, answer, ideal_answer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8b64b",
   "metadata": {},
   "source": [
    "Text based Chatbot(ASk question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4872719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store from embeddings.parquet\n",
      "\n",
      "Question:\n",
      " What is the rule for uncontrolled pedestrian crossing?\n",
      "\n",
      "Answer:\n",
      " Drivers must yield to pedestrians who have already stepped onto the uncontrolled pedestrian crossing or are about to do so.\n",
      "\n",
      "Context Sentences:\n",
      "- This means that the pedestrian crossing is uncontrolled. (Page 47, Sentence 4)\n",
      "- An uncontrolled pedestrian crossing. (Page 47, Sentence 0)\n",
      "- Uncontrolled pedestrian crossings\n",
      "Drivers have an obligation to give way to pedestrians who have stepped\n",
      "out onto the pedestrian crossing or who are about to do so. (Page 46, Sentence 12)\n",
      "- Controlled pedestrian crossings\n",
      "Have functioning traffic signals (or a police officer). (Page 46, Sentence 8)\n",
      "- Is it prohibited\n",
      "to overtake the bus in conjunction with the pedestrian\n",
      "crossing? (Page 105, Sentence 1)\n"
     ]
    }
   ],
   "source": [
    "store = load_vector_store(\"embeddings.parquet\")  # Load previously saved vectors\n",
    "\n",
    "# Ask one question per cell execution\n",
    "question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "if question.lower() != \"exit\":\n",
    "    query_vector = create_embeddings(question)\n",
    "    top_sentences = store.semantic_search(query_vector, k=5)\n",
    "    answer = generate_answer(question, top_sentences)\n",
    "\n",
    "    print(\"\\nQuestion:\\n\", question)\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "    print(\"\\nContext Sentences:\")\n",
    "    for s in top_sentences:\n",
    "        print(f\"- {s['text']} (Page {s['meta']['page']}, Sentence {s['meta']['index']})\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d903077",
   "metadata": {},
   "source": [
    "Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f3071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Critical Reflection\n",
    "\n",
    "# API Key Usage:\n",
    "# This chatbot uses an API key stored as an environment variable (api_key= os.getenv(\"API_KEY\")) for authentication.\n",
    "# This method retrieves the API key from environment variables to authenticate the request securely.\n",
    "# The API key is required for communicating with external services (e.g., AI models, vector databases).\n",
    "\n",
    "# Real-World Application:\n",
    "# This chatbot acts as an AI-powered study assistant specifically for Swedish driving theory learners.\n",
    "# It retrieves information directly from the \"Introduction\" section of the 2025 Driving Theory Book,\n",
    "# ensuring learners get precise and structured explanations about foundational driving concepts.\n",
    "\n",
    "# Challenges & Opportunities\n",
    "# Business Perspective:\n",
    "# Can be integrated into official driving school platforms as a self-study tool.\n",
    "# Provides automated assistance, reducing the need for manual responses from instructors.\n",
    "# Could evolve into a paid application for learners preparing for driving exams.\n",
    "\n",
    "# Ethical Perspective:\n",
    "# Users must be aware that it is an educational tool, not a legal authority on driving laws.\n",
    "# Since only the introduction section is covered, learners must verify details from the full book.\n",
    "# Bias in responses should be monitored to ensure clear, unbiased driving guidance.\n",
    "\n",
    "# Technical Perspective:\n",
    "# The chatbot relies on sentence-based chunking, embeddings, and semantic search to retrieve relevant details.\n",
    "# Vector storage allows efficient searching within the introduction section.\n",
    "# Performance could improve by expanding coverage beyond the introduction for a full-driving theory assistant.\n",
    "\n",
    "# Future Possibilities:\n",
    "# Support for multi-chapter processing, expanding to cover all sections of the driving theory book including image.\n",
    "# Voice input integration to make it accessible as an interactive AI driving tutor.\n",
    "# Currently, the chatbot focuses only on introductory driving concepts, but future improvements could transform it into a\n",
    "# fully functional AI tutor for learners preparing for the driving exam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
